{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237bb28a-58a4-4629-a934-50597ddc7e76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fffa13c",
   "metadata": {},
   "source": [
    "# 2022/23 NBA Drafts Team Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca74a0",
   "metadata": {},
   "source": [
    "Data on Teams:\n",
    "- https://www.nba.com/draft/2022/team-profiles\n",
    "- https://www.nba.com/stats/teams and https://www.nba.com/stats/teams/traditional includes\n",
    "    - Season Leaders\n",
    "    - Advanced\n",
    "    - Miscellaneous\n",
    "    - Player Tracking Speed\n",
    "    - Hustle\n",
    "    - Scoring\n",
    "    - Shooting by Zone\n",
    "    - Bench\n",
    "    - Stats in Wins\n",
    "    - Defense\n",
    "-https://www.basketball-reference.com/teams/\n",
    "\n",
    "Data on Players:\n",
    "- https://www.nba.com/stats/players and https://www.nba.com/stats/leaders includes:\n",
    "    - Season Leaders\n",
    "    - Advanced\n",
    "    - Miscellaneous\n",
    "    - Scoring\n",
    "    - Shooting By Zone\n",
    "    - Bio Stats\n",
    "    - Centers\n",
    "    - Forwards\n",
    "    - Guards\n",
    "    - 1st Overall Picks\n",
    "    - Rookies\n",
    "    - Active Career Leaders\n",
    "- https://www.nba.com/stats/draft/combine\n",
    " - Standing reach\n",
    " - Height w/o shoes\n",
    " - Wingspan\n",
    " - Standing vertical leap\n",
    " - Max vertical leap\n",
    " - Shuttle run\n",
    " - Lane agility\n",
    " - Three quarter sprint\n",
    "-https://www.basketball-reference.com/players/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b71ef",
   "metadata": {},
   "source": [
    "## Program Objectives\n",
    "\n",
    "1. Find 2021/22 Team Statistics\n",
    "2. Find 2022's new draft picks with their corresponding characteristics and new teams\n",
    "3. Prediction the next season's results based on the above\n",
    "4. Compare results with 2022/23 Team Statistics\n",
    "5. Answer the Focus Question: Is the NBA competatively balanced? What is the impact of the best players drafted into the worst performing teams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77b788-e6ff-44c9-8f8e-f05874955f05",
   "metadata": {},
   "source": [
    "## 1. 21/22 Team Stats\n",
    "## 2. 2022 Draft Picks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989be880-703e-4979-8249-d19e49c4ceae",
   "metadata": {},
   "source": [
    "Lets try pandas to read the table on nba drafted players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150575d3-357d-426f-acdf-d31ef1ab7296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "                                              0.0/112.2 kB ? eta -:--:--\n",
      "     --------------------------------        92.2/112.2 kB 2.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 112.2/112.2 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\thops\\appdata\\roaming\\python\\python311\\site-packages (from html5lib) (1.16.0)\n",
      "Collecting webencodings (from html5lib)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: webencodings, html5lib\n",
      "Successfully installed html5lib-1.1 webencodings-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d7336-9fec-4853-98e3-6d34db33a56c",
   "metadata": {},
   "source": [
    "### Webscraping of Player Data\n",
    "\n",
    "Here data on the draft player name's, their new teams, affliations and overall pickings rank are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b45cc9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArrowDtype', 'BooleanDtype', 'Categorical', 'CategoricalDtype', 'CategoricalIndex', 'DataFrame', 'DateOffset', 'DatetimeIndex', 'DatetimeTZDtype', 'ExcelFile', 'ExcelWriter', 'Flags', 'Float32Dtype', 'Float64Dtype', 'Grouper', 'HDFStore', 'Index', 'IndexSlice', 'Int16Dtype', 'Int32Dtype', 'Int64Dtype', 'Int8Dtype', 'Interval', 'IntervalDtype', 'IntervalIndex', 'MultiIndex', 'NA', 'NaT', 'NamedAgg', 'Period', 'PeriodDtype', 'PeriodIndex', 'RangeIndex', 'Series', 'SparseDtype', 'StringDtype', 'Timedelta', 'TimedeltaIndex', 'Timestamp', 'UInt16Dtype', 'UInt32Dtype', 'UInt64Dtype', 'UInt8Dtype', '__all__', '__builtins__', '__cached__', '__doc__', '__docformat__', '__file__', '__git_version__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_built_with_meson', '_config', '_is_numpy_dev', '_libs', '_pandas_datetime_CAPI', '_pandas_parser_CAPI', '_testing', '_typing', '_version_meson', 'annotations', 'api', 'array', 'arrays', 'bdate_range', 'compat', 'concat', 'core', 'crosstab', 'cut', 'date_range', 'describe_option', 'errors', 'eval', 'factorize', 'from_dummies', 'get_dummies', 'get_option', 'infer_freq', 'interval_range', 'io', 'isna', 'isnull', 'json_normalize', 'lreshape', 'melt', 'merge', 'merge_asof', 'merge_ordered', 'notna', 'notnull', 'offsets', 'option_context', 'options', 'pandas', 'period_range', 'pivot', 'pivot_table', 'plotting', 'qcut', 'read_clipboard', 'read_csv', 'read_excel', 'read_feather', 'read_fwf', 'read_gbq', 'read_hdf', 'read_html', 'read_json', 'read_orc', 'read_parquet', 'read_pickle', 'read_sas', 'read_spss', 'read_sql', 'read_sql_query', 'read_sql_table', 'read_stata', 'read_table', 'read_xml', 'reset_option', 'set_eng_float_format', 'set_option', 'show_versions', 'test', 'testing', 'timedelta_range', 'to_datetime', 'to_numeric', 'to_pickle', 'to_timedelta', 'tseries', 'unique', 'util', 'value_counts', 'wide_to_long']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List all attributes in the pandas module\n",
    "print(dir(pd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2711851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConnectTimeout', 'ConnectionError', 'DependencyWarning', 'FileModeWarning', 'HTTPError', 'JSONDecodeError', 'NullHandler', 'PreparedRequest', 'ReadTimeout', 'Request', 'RequestException', 'RequestsDependencyWarning', 'Response', 'Session', 'Timeout', 'TooManyRedirects', 'URLRequired', '__author__', '__author_email__', '__build__', '__builtins__', '__cached__', '__cake__', '__copyright__', '__description__', '__doc__', '__file__', '__license__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__title__', '__url__', '__version__', '_check_cryptography', '_internal_utils', 'adapters', 'api', 'auth', 'certs', 'chardet_version', 'charset_normalizer_version', 'check_compatibility', 'codes', 'compat', 'cookies', 'delete', 'exceptions', 'get', 'head', 'hooks', 'logging', 'models', 'options', 'packages', 'patch', 'post', 'put', 'request', 'session', 'sessions', 'ssl', 'status_codes', 'structures', 'urllib3', 'utils', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# List all attributes in the requests module\n",
    "print(dir(requests))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff72d88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dialect', 'DictReader', 'DictWriter', 'Error', 'QUOTE_ALL', 'QUOTE_MINIMAL', 'QUOTE_NONE', 'QUOTE_NONNUMERIC', 'Sniffer', 'StringIO', '_Dialect', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', 'excel', 'excel_tab', 'field_size_limit', 'get_dialect', 'list_dialects', 're', 'reader', 'register_dialect', 'unix_dialect', 'unregister_dialect', 'writer']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# List all attributes in the bs4 module\n",
    "print(dir(csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57de5fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASCII_SPACES', 'DEFAULT_BUILDER_FEATURES', 'DEFAULT_INTERESTING_STRING_TYPES', 'EMPTY_ELEMENT_EVENT', 'END_ELEMENT_EVENT', 'NO_PARSER_SPECIFIED_WARNING', 'ROOT_TAG_NAME', 'START_ELEMENT_EVENT', 'STRING_ELEMENT_EVENT', '__bool__', '__call__', '__class__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_all_strings', '_clone', '_decode_markup', '_event_stream', '_feed', '_find_all', '_find_one', '_format_tag', '_indent_string', '_is_xml', '_lastRecursiveChild', '_last_descendant', '_linkage_fixer', '_markup_is_url', '_markup_resembles_filename', '_popToTag', '_should_pretty_print', 'append', 'childGenerator', 'children', 'clear', 'css', 'decode', 'decode_contents', 'decompose', 'decomposed', 'default', 'descendants', 'encode', 'encode_contents', 'endData', 'extend', 'extract', 'fetchNextSiblings', 'fetchParents', 'fetchPrevious', 'fetchPreviousSiblings', 'find', 'findAll', 'findAllNext', 'findAllPrevious', 'findChild', 'findChildren', 'findNext', 'findNextSibling', 'findNextSiblings', 'findParent', 'findParents', 'findPrevious', 'findPreviousSibling', 'findPreviousSiblings', 'find_all', 'find_all_next', 'find_all_previous', 'find_next', 'find_next_sibling', 'find_next_siblings', 'find_parent', 'find_parents', 'find_previous', 'find_previous_sibling', 'find_previous_siblings', 'format_string', 'formatter_for_name', 'get', 'getText', 'get_attribute_list', 'get_text', 'handle_data', 'handle_endtag', 'handle_starttag', 'has_attr', 'has_key', 'index', 'insert', 'insert_after', 'insert_before', 'isSelfClosing', 'is_empty_element', 'known_xml', 'new_string', 'new_tag', 'next', 'nextGenerator', 'nextSibling', 'nextSiblingGenerator', 'next_elements', 'next_siblings', 'object_was_parsed', 'parentGenerator', 'parents', 'parserClass', 'popTag', 'prettify', 'previous', 'previousGenerator', 'previousSibling', 'previousSiblingGenerator', 'previous_elements', 'previous_siblings', 'pushTag', 'recursiveChildGenerator', 'renderContents', 'replaceWith', 'replaceWithChildren', 'replace_with', 'replace_with_children', 'reset', 'select', 'select_one', 'self_and_descendants', 'setup', 'smooth', 'string', 'string_container', 'strings', 'stripped_strings', 'text', 'unwrap', 'wrap']\n"
     ]
    }
   ],
   "source": [
    "# run this line \"pip install bs4\" in the command window \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List all attributes in the bs4 module\n",
    "print(dir(BeautifulSoup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8059a21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XlsxWriter' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\thops\\OneDrive\\Desktop\\2023\\github projects\\NBA Drafted Player Impact\\NBA Drafted Player Impact.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thops/OneDrive/Desktop/2023/github%20projects/NBA%20Drafted%20Player%20Impact/NBA%20Drafted%20Player%20Impact.ipynb#X16sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m df\u001b[39m.\u001b[39mto_excel(excel_writer, sheet_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplayer data\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thops/OneDrive/Desktop/2023/github%20projects/NBA%20Drafted%20Player%20Impact/NBA%20Drafted%20Player%20Impact.ipynb#X16sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Close the Pandas Excel writer and save the Excel file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/thops/OneDrive/Desktop/2023/github%20projects/NBA%20Drafted%20Player%20Impact/NBA%20Drafted%20Player%20Impact.ipynb#X16sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m excel_writer\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/thops/OneDrive/Desktop/2023/github%20projects/NBA%20Drafted%20Player%20Impact/NBA%20Drafted%20Player%20Impact.ipynb#X16sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'XlsxWriter' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "\n",
    "#url = \"https://www.nba.com/stats/teams\"\n",
    "url = \"https://www.nba.com/stats/draft/history?Season=2022\"\n",
    "\n",
    "page_to_scrape = requests.get(url)\n",
    "soup = BeautifulSoup(page_to_scrape.text,\"html.parser\")\n",
    "\n",
    "players = soup.findAll(\"td\", attrs={\"class\":\"Crom_text__NpR1_ Crom_sticky__uYvkp Crom_player__BuOU9\"}) # so the tag is \"td\" and the class attribute is \"Crom_text__NpR1_ Crom_sticky__uYvkp Crom_player__BuOU9\"\n",
    "#<td class=\"Crom_text__NpR1_ Crom_sticky__uYvkp Crom_player__BuOU9\"><a href=\"/stats/player/1631216/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Caleb Houstan</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_sticky__uYvkp Crom_player__BuOU9\"><a href=\"/stats/player/1631107/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Nikola Jovic</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_sticky__uYvkp Crom_player__BuOU9\"><a href=\"/stats/player/1631100/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">AJ Griffin</a></td>\n",
    "\n",
    "teams = soup.findAll(\"a\", attrs={\"class\":\"Anchor_anchor__cSc3P\"})\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612739/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Cleveland Cavaliers</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612754/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Indiana Pacers</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612747/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Los Angeles Lakers</a></td>\n",
    "\n",
    "#affiliations\n",
    "#round_numbers\n",
    "#round_picks\n",
    "#overall_picks\n",
    "\n",
    "file = open(\"here.csv\", \"w\")\n",
    "writer = csv.writer(file)\n",
    "\n",
    "writer.writerow([\"Player\",\"Team\"])\n",
    "\n",
    "for player, team in zip(players, teams):\n",
    "    print(player.text + \" - \" + team.text)\n",
    "    writer.writerow([player.text,team.text])\n",
    "\n",
    "file.close()\n",
    "\n",
    "#######################3\n",
    "# Assuming you have already fetched the HTML content and created a BeautifulSoup object named 'soup'\n",
    "\n",
    "# Find all <td> elements with the specified class\n",
    "td_elements = soup.find_all(\"td\", class_=\"Crom_text__NpR1_ Crom_primary__EajZu\")\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612739/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Cleveland Cavaliers</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612754/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Indiana Pacers</a></td>\n",
    "#<td class=\"Crom_text__NpR1_ Crom_primary__EajZu\"><a href=\"/stats/team/1610612747/\" class=\"Anchor_anchor__cSc3P\" data-is-external=\"false\" data-has-more=\"false\" data-has-children=\"true\">Los Angeles Lakers</a></td>\n",
    "\n",
    "# Initialize an empty list to store the team names\n",
    "team_names = []\n",
    "\n",
    "# Iterate through the <td> elements and extract the team names\n",
    "for td in td_elements:\n",
    "    # Find the <a> tag within the <td> element and extract its text\n",
    "    a_tag = td.find(\"a\", class_=\"Anchor_anchor__cSc3P\")\n",
    "    if a_tag:\n",
    "        team_name = a_tag.text.strip()  # Extract and strip the text\n",
    "        team_names.append(team_name)\n",
    "\n",
    "# Now, 'team_names' should contain a list of team names\n",
    "print(team_names)\n",
    "\n",
    "\n",
    "# Create a CSV file and write the data into it\n",
    "with open(\"scraped_nba_data.csv\", \"w\", newline=\"\") as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    \n",
    "    # Write the header row (if needed)\n",
    "    csv_writer.writerow([\"Team\"])  # Header row\n",
    "    \n",
    "    # Write the team names\n",
    "    for team_name in team_names:\n",
    "        csv_writer.writerow([team_name])\n",
    "\n",
    "# Create an Excel file with a sheet named \"player data\" and write the data into it\n",
    "df = pd.DataFrame({\"Team\": team_names})\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine\n",
    "excel_writer = pd.ExcelWriter(\"scraped_nba_data.xlsx\", engine=\"xlsxwriter\")\n",
    "\n",
    "# Write the DataFrame to the Excel file\n",
    "df.to_excel(excel_writer, sheet_name=\"player data\", index=False)\n",
    "\n",
    "# Close the Pandas Excel writer and save the Excel file\n",
    "#excel_writer.save()\n",
    "\n",
    "df.head()\n",
    "# List all attributes in the bs4 module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4ee69c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__abstractmethods__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__fspath__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_book', '_cur_sheet', '_date_format', '_datetime_format', '_docstring_components', '_engine', '_get_sheet_name', '_handles', '_if_sheet_exists', '_is_protocol', '_mode', '_path', '_save', '_supported_extensions', '_value_with_fmt', '_write_cells', 'book', 'check_extension', 'close', 'date_format', 'datetime_format', 'engine', 'if_sheet_exists', 'sheets', 'supported_extensions']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Team]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(excel_writer))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7a3ba6-a3fe-4e0e-aa7d-da00b4159ef1",
   "metadata": {},
   "source": [
    "Lets try reading the table with beautiful soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7096f-aafb-4592-a06d-c225d5d03e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279de92-c689-4630-bcba-a418bbe6461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2000,2022))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd2f900-f84f-4ad2-808b-3b3c8a46d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_start = \"https://www.nba.com/stats/draft/history?Season={}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e343aeb-f002-420b-b886-342af9c41609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for year in years:\n",
    "    url = url_start.format(year)\n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"drafts/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f1011e-d2fb-45a8-b289-594bd7d82b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The above method saves the websites, but without the actual data on drafted players and their teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6304554-df15-42b6-8f41-2ec669e832f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.nba.com/stats/draft/history?Season=2021\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "table = soup.find(\"table\")\n",
    "rows = table.find_all(\"tr\")\n",
    "header = [th.text.strip() for th in rows[0].find_all(\"th\")]\n",
    "data = []\n",
    "for row in rows[1:]:\n",
    "    cells = [td.text.strip() for td in row.find_all(\"td\")]\n",
    "    data.append(dict(zip(header, cells)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e020d8-fc3e-490a-a4c9-d934f894f0d4",
   "metadata": {},
   "source": [
    "Lets try pandas again, but with a different url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729af21-ed98-44b3-b651-959bc64ba04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://en.hispanosnba.com/nba/history/draft/seasons\" #this table gives top three picks for each year\n",
    "\n",
    "# Use pandas' `read_html` function to extract tables from the website\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Select the desired table from the list of tables\n",
    "df1 = tables[0] #top picks from 2010 to 2022\n",
    "df2 = tables[1] #top picks from 2000 to 2009\n",
    "\n",
    "# Concatenate the two DataFrames vertically\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Reset the index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Show the first 5 rows of the dataframe\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "print(\". . .\")\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c476598-eb7b-4d7d-9176-107858ea12b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see if we can get more than the top 3 picks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69acf803-32a2-45b9-8d21-b753ce69991f",
   "metadata": {},
   "source": [
    "## Team Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c035d-f0e6-47b5-9560-1ade6a60739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "year = list(range(2000,2023))\n",
    "\n",
    "url_start = \"https://www.basketball-reference.com/leagues/NBA_{}.html\"\n",
    "\n",
    "for year in years:\n",
    "    url = url_start.format(year)\n",
    "    data = requests.get(url)\n",
    "    \n",
    "    with open(\"teams/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8719cb2-5f5d-403d-b1d5-36e7d7c1c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021/22 season is not included with the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "82714653-d3d8-46fd-ac1c-b30f7017ee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fe4ecd8-9317-4890-968c-4d21f9fafe33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from html5lib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d4a691bf-a56d-4d4e-b52a-abc73c645a8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "html5lib not found, please install it",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_366/577791118.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Use pandas' `read_html` function to extract tables from the website\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Select the desired table from the list of tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only)\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mna_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mkeep_default_na\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_default_na\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplayed_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m     )\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m     \u001b[0mretained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflav\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflavor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflav\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_match\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplayed_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/conda/envs/python/lib/python3.7/site-packages/pandas/io/html.py\u001b[0m in \u001b[0;36m_parser_dispatch\u001b[0;34m(flavor)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflavor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"bs4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html5lib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAS_HTML5LIB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"html5lib not found, please install it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_HAS_BS4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BeautifulSoup4 (bs4) not found, please install it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: html5lib not found, please install it"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.nba.com/standings?Season=2021-22\"\n",
    "\n",
    "# Use pandas' `read_html` function to extract tables from the website\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "# Select the desired table from the list of tables\n",
    "standings = tables[0]\n",
    "\n",
    "# Show the first 5 rows of the dataframe\n",
    "print(standings.shape)\n",
    "print(standings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996593f-ee67-48c8-8dff-bd57121ba2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857229d8-53b1-4e66-9238-d79ab838372e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
